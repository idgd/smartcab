QUESTION: Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?
When it's totally random, it seems to make its way to the destination more than half the time, but it hits the hard time limit of -100 sometimes as well. As it's entirely random, it does not care about traffic or the lights, making it an incredibly dangerous driver. Unless every other driver on the road is incredibly skilled, this car is not likely to make it to its destination in one piece!

QUESTION: What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?
Almost all of the input is logically necessary for the state to function, except for the deadline variable. Because it changes depending on the number of moves made, it makes for a poor teacher as far as state goes. The learner is very likely to encounter the same combination of the other variables (oncoming, left, right, and light), but rarely will it encounter the same combination of the others *and* the deadline, making for an exceedingly poor learner. Kind of an equivalent to overfitting. However, adding on one more variable, next_waypoint, should be a good thing, because we want the qlearner to associate good things with getting closer to the target position.

QUESTION: How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?
The 1st, 3rd, 4th, and 5th variables all have 4 possible states. The 2nd variable has 2 possible states. So, the number of possible states should be 4^4 * 2^2, or 1024 possible states.

QUESTION: What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?
I notice that it fails a lot less. On top of that, it gets better over time; on the 1st run, it's not much better than a random actor, but as it goes on, it improves until it's near perfect. This behaviour is occurring because it's learning the appropriate action for every possible scenario, and takes into account the next_waypoint, which will always points to the best way to get to the destination.
